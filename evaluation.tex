\documentclass[dissertation.tex]{subfiles}

\begin{document}

\section{Success Criteria}
{

    The success criteria laid out in the project proposal have all been satisfied:

    \begin{itemize}
    \item Translate simple Haskell programs into executable Java bytecode.
    \item Reject ill-formed programs due to syntactic or type errors.
    \item Perform simple optimisations during translation.
    \item Perform evaluation using non-strict semantics.
    \end{itemize}
    
    Beyond these base requirements, a number of the extensions implementing language features have been completed
    successfully.

    \todo[inline]{Mention somewhere pros/cons of using JVB}
}
\section{Language Features}
{

    The planned subset of Haskell was functions, arithmetic, booleans, lists, simple typeclasses, and laziness. The
    following program demonstrates various use-cases of all of these features:

    \begin{haskellfigure}
    -- foldl :: (b -> a -> b) -> b -> [a] -> b
    foldl _ e [] = e
    foldl f e (x:xs) = foldl f (f e x) xs

    -- sum :: Num a => [a] -> a
    sum = foldl (+) 0

    -- take :: Int -> [a] -> [a]
    take 0 _ = []
    take _ [] = undefined
    take n (x:xs) = x:take (n-1) xs

    -- ones :: Num a => [a]
    ones = 1:ones

    -- valid :: Bool
    valid = sum (take 10 ones :: [Int]) == 10
    \end{haskellfigure}

    Successfuly implemented extensions include support for user-defined datatypes, user-defined typeclasses and
    instances, monads, and some syntactic features like operator sections and support for point-free notation: these can
    be demonstrated by the following program:

    \todo[inline]{Hard to give an example program using monads without ending up doing a monad tutorial!}
    \begin{haskellfigure}
    data Maybe a = Nothing | Just a
    data [] a = [] | a:[a]
    
    class Monad m where
        (>>=) :: m a -> (a -> m b) -> m b
        return :: a -> m a
    instance Monad Maybe where
        Nothing >>= f = Nothing
        (Just x) >>= f = f x
        return = Just
    instance Monad [] where
        [] >>= f = []
        (x:xs) >>= f = (f x) ++ (f >>= xs)
        return x = [x]

    -- The monad instance for maybe can be interpreted as function application
    -- with support for chaining failure
    divide x y = if y == 0 then Nothing else Just (x / y)
    x = divide 4 0 >>= divide 20 -- Evaluates to Nothing

    -- The monad instance for lists can be interpreted as performing
    -- non-deterministic computation: each step can have multiple results
    countdown 0 = []
    countdown n = n:countdown (n - 1)
    onlyEven x = if even x then [x] else []
    y = [1,2,3] >>= countdown -- Evaluates to [1,2,1,3,2,1]
    z = y >>= onlyEven -- Evaluates to [2,2]
    \end{haskellfigure}

    \todo[inline]{Double triple check these programs work (add to tests) as it'd be really awkward if someone tries to test them and they don't...}

    Each simple feature name necessarily glosses over many smaller constituent features necessary for use. For example,
    the `lists' feature allows for lists to be created using either the plain constructor syntax
    (\haskell{1:(2:(3:[]))}) or syntactic sugar for lists (\haskell{[1,2,3]}), and matched using patterns (eg.\
    \haskell{[x,y] = [1,2]}). However, there's no support for list comprehensions (eg.\ \haskell{[f x | x <- [1,2,3],
    even x]}) as they weren't a high priority feature.

    \todo[inline]{This \^ paragraph is kinda trying to explain that there's a lot of things associated with each coarse `feature', and that I've implemented all the obvious ones but that there are other aspects I won't have implemented. Unnecessary?}

    \todo[inline]{Should I talk about missing features? Typeclass/instance superclasses, default method implementations in classes, symbol type signatures...}
    
    \subsubsection{Correctness}
    {

        Correctness of the various stages of the compiler has been empirically tested using a large set of unit and
        integration tests: these include tests of complete programs, such as those used for benchmarking.
        
        At the time of writing, there are 245\todo{Update} tests. These are run both on my development machine
        (described in \ref{sec:test-environment}), and on machines provided by Travis
        CI\footnote{\url{https://travis-ci.org/hnefatl/dissertation-project}} whenever a commit is pushed to my
        development GitHub repository. This ensures the compiler works in a clean, reproducible environment and not just
        on my development system.

        As the compiler is developed using Haskell, some forms of compiler bugs that could affect the correctness of
        translation have been mitigated: type errors and bugs due to mutable state cannot exist within the compiler.
        
    }
}
\section{Performance}
{

    Although performance was not an important aspect of the success criteria, it's still interesting to evaluate the
    effectiveness of optimisations on the output program, and the performance of the compiler and its output compared to
    other Haskell to JVB compilers.

    The two compilers used for comparisons are Eta and Frege. Eta is a fork of GHC that replaces the backend with one
    targeting JVB: it can take advantage of the powerful optimisations already available in the front- and middle-end of
    GHC, which is the world-leading Haskell compiler, so I expect it to perform better than both my compiler and Frege.
    Frege is a from-scratch compiler that compiles to Java instead of JVB, then uses a Java compiler to produce JVB. I
    expect Frege to perform better than my compiler, given the maturity of the project (under development since at least
    2011).

    Both Eta and Frege enable optimisations by default, so all metrics given for them have optimisations enabled.
    Metrics given for my compiler are labelled to indicate whether or not optimisations have been applied.

    \subsection{Test Environment}\label{sec:test-environment}
    {

        Benchmarks were performed as the only active process on my development machine: a ThinkPad 13 running Debian 9
        with 8GB RAM and an Intel Core i5-7200U CPU (2.5GHz).

    }
    \subsection{Benchmark Approach}
    {

        All of the compilers being compared output JVB, so a natural choice of benchmarking framework was the Java
        Microbenchmark Harness (JMH)\todo{cite}. This allows for accurate Java program benchmarking by handling JVM
        warmup, disabling garbage collection, etc. There are downsides to the framework though, notably that it doesn't
        record memory usage, and that it only appears to expose percentiles and histograms of the results through its
        Java API, not the raw data.

        To measure compiler performance, a more naÃ¯ve approach was taken: the execution times of 50 sequential
        compilations were recorded for each compiler.
        
        For my compiler, the time taken to write the compiled class files to disk and compress them into a jar file was
        computed by measuring the difference between compilation runs which write to disk, and those which don't (using
        the \monospace{--no-write-jar} command-line-flag). This data is used in Figure \ref{fig:compiler-perf}.

    }
    \subsection{Execution Speed}
    {

        Figure \ref{fig:perf} demonstrates the runtime performance of the benchmark programs after compilation by the
        different compilers. It is evident that the performance of programs compiled by my compiler is significantly
        lower than those from Frege or Eta, but also that applying optimisations can produce a reasonable speedup.

        Interestingly Frege produced more performant programs than Eta, which wasn't expected.

        \begin{figure}[h]
            \centering
            \captionsetup{width=0.8\textwidth}
            \includegraphics[width=0.9\textwidth]{graphs/perf.pdf}
            \caption{Median runtime in milliseconds of the benchmark programs. Error bars show 25th and 75th percentiles.}
            \label{fig:perf}
        
            \todo[inline]{Update to use min with error bars showing 25th+50th quartile or similar}
        \end{figure}

        % Why is mine slower. Ideally include a profiling plot or something.

        % Can conclude that my implementation of thunks is inefficient, compare to Frege/Eta's implementation. Think
        % they both use loads of anonymous classes for their implementations while mine avoids that: can frame it as `I
        % tried a different approach and found it was less performant', not necessarily a bad thing as speed was never a
        % focus, and make guesses (and call them hypotheses) as to why their version is better.

    }
    \subsection{Compiler Performance}
    {

        Figure \ref{fig:compiler-perf} presents the time taken to compile each benchmark program. 
        
        \begin{figure}[h]
            \centering
            \captionsetup{width=0.8\textwidth}
            \includegraphics[width=0.9\textwidth]{graphs/compiler_perf.pdf}
            \caption{Minimum time taken to compile the benchmark program. Dark grey section for the times from my compiler indicates the time taken writing the compiled classes to a compressed Jar file.}
            \label{fig:compiler-perf}
        \end{figure}

    }

}
\section{Schedule}
{

    \todo[inline]{This is a WIP rehash of the stuff from the progress report explaining why the schedule got messed up, I feel it'll help explain why optimisations didn't get much splotlight.}

    The core features of Haskell are tightly coupled: simple features such as arithmetic operators require a significant
    level of support for other language features. For example, the \haskell{(+)} function relies on:

    \begin{description}
    \item[Typeclasses:] \haskell{(+)} is defined by the \haskell{Num} typeclass in order to allow ad-hoc overloading.
    \item[Typeclass instances:]
    {
        The types that can be used as arguments to the overloaded functions, and the implementation of the overloads,
        are defined by typeclass instances.
    }
    \item[Datatypes:]
    {
        The most common implementation of typeclasses involves translating classes into datatypes and instances into
        values of the datatype.
    }
    \end{description}

    It would be possible to implement a function like \haskell{(+)} which only worked for integers and avoid all of the
    dependencies on other language features, but then the language simply wouldn't be Haskell: it would resemble a lazy
    variant of ML's semantics.

    All of these language features are very expensive to implement, as they span multiple layers of the compiler: the
    type checker needs to be able to infer and check types based on the usage of these features, they need to be
    translatable into intermediate languages, and the code generator needs to be able to produce bytecode reflecting the
    semantics.

}




\end{document}